Logistic Regression Spam Classifier Review

Overview:
This project implements a binary email spam classifier using a manually-coded logistic regression model, relying solely on NumPy for matrix operations. The classifier is trained on a labeled dataset of email messages, using gradient descent for optimization. It provides a lightweight and interpretable baseline for spam detection with fully transparent computations and no external ML libraries.

Structure:

 - Input Layer: 54 features (email attributes from the dataset)
 - Output Layer: 1 node (sigmoid activation for binary classification)
 - Model Type: Binary Logistic Regression
 
Forward Propagation Formula:
    z = -(W · X + b)
    ŷ = sigmoid(z)
Where:
 - W = weight vector
 - X = input feature matrix
 - b = bias scalar
 - ŷ = predicted probability of spam
 
Activation Function:
 - Sigmoid (used in the output layer):
       Sigmoid(x) = 1 / (1 + exp(-x))
   This activation compresses output values to the range (0, 1), allowing the model to interpret them as probabilities for binary classification.
   
Loss Function:
 - Binary Cross Entropy (implicitly minimized through gradient descent):
       Loss = -(1/N) * Σ [ y * log(ŷ) + (1 - y) * log(1 - ŷ) ]
   While not explicitly coded, this is the theoretical loss being optimized via the gradients.
   
Training and Optimization:
 - Gradient Descent is used to update weights and bias iteratively:
    - Weight update:
      W = W - learning_rate * dw
    - Bias update:
      b = b - learning_rate * db

 - Gradients are calculated on the full dataset (batch gradient descent).
 - The model uses a default of 2500 iterations and a learning rate of 0.1.
 
Considerations:
 - Dataset size and feature dimensions are assumed based on the format of training_spam.csv.
 - Model is interpretable and lightweight, but may underperform on more complex patterns without non-linear transformations.
 - Logistic regression is effective for linearly separable data but may struggle with noisy or overlapping classes.
 
Future Improvements:
 - Explicit implementation of a loss tracking mechanism during training
 - Integration of regularization (L1 or L2) to prevent overfitting
 - Extension to mini-batch or stochastic gradient descent
 - Incorporation of feature scaling and preprocessing
 - Evaluation on a separate test set for generalization performance
 - Comparison against more complex models (e.g., neural networks, tree-based methods)
 
Conclusion:
This project demonstrates a clear and educational implementation of a logistic regression spam filter from scratch using NumPy. It offers a transparent view of how classification models work internally and lays the groundwork for more advanced approaches. Its simplicity and accessibility make it ideal for foundational learning in machine learning and model development.

